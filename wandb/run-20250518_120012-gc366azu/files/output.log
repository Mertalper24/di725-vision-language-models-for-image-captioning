Loading dataset...
Initializing model with QLoRA...
`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.
Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use
`config.hidden_activation` if you want to override this behaviour.
See https://github.com/huggingface/transformers/pull/29402 for more details.
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:08<00:00,  3.00s/it]
trainable params: 11,298,816 || all params: 2,934,765,296 || trainable%: 0.3850
C:\Users\Mert\AppData\Local\Programs\Python\Python39\lib\site-packages\transformers\training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead
  warnings.warn(
Starting training...
[34m[1mwandb[0m: [33mWARNING[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 24/24 [20:08<00:00, 50.33s/it]              
{'loss': 16.1822, 'grad_norm': 7.044061660766602, 'learning_rate': 2e-05, 'epoch': 0.17}
{'loss': 16.3004, 'grad_norm': 5.809993743896484, 'learning_rate': 1.8181818181818182e-05, 'epoch': 0.33}
{'loss': 16.6478, 'grad_norm': 5.602094650268555, 'learning_rate': 1.6363636363636366e-05, 'epoch': 0.5}
{'loss': 17.1767, 'grad_norm': 4.514334201812744, 'learning_rate': 1.4545454545454546e-05, 'epoch': 0.67}
{'loss': 18.2093, 'grad_norm': 6.504497051239014, 'learning_rate': 1.2727272727272728e-05, 'epoch': 0.83}
{'loss': 17.0968, 'grad_norm': 8.4967622756958, 'learning_rate': 1.0909090909090909e-05, 'epoch': 1.0}
{'loss': 16.5682, 'grad_norm': 5.89993953704834, 'learning_rate': 9.090909090909091e-06, 'epoch': 1.17}
{'loss': 16.1522, 'grad_norm': 5.616613864898682, 'learning_rate': 7.272727272727273e-06, 'epoch': 1.33}
{'loss': 17.2309, 'grad_norm': 6.793606281280518, 'learning_rate': 5.4545454545454545e-06, 'epoch': 1.5}
{'loss': 17.1065, 'grad_norm': 7.096584796905518, 'learning_rate': 3.6363636363636366e-06, 'epoch': 1.67}
{'loss': 15.383, 'grad_norm': 31.16341781616211, 'learning_rate': 1.8181818181818183e-06, 'epoch': 1.83}
{'loss': 16.5191, 'grad_norm': 6.780458927154541, 'learning_rate': 0.0, 'epoch': 2.0}
{'train_runtime': 1208.022, 'train_samples_per_second': 0.157, 'train_steps_per_second': 0.02, 'train_loss': 16.71444304784139, 'epoch': 2.0}
Saving model...
