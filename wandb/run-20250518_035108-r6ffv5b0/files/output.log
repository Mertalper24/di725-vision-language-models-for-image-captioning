Loading dataset...
Initializing model with QLoRA...
`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.
Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use
`config.hidden_activation` if you want to override this behaviour.
See https://github.com/huggingface/transformers/pull/29402 for more details.
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:23<00:00,  7.92s/it]
trainable params: 11,298,816 || all params: 2,934,765,296 || trainable%: 0.3850
C:\Users\Mert\AppData\Local\Programs\Python\Python39\lib\site-packages\transformers\training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead
  warnings.warn(
Starting training...
Traceback (most recent call last):
  File "C:\Users\Mert\Desktop\di725-vision-language-models-for-image-captioning\train_with_wandb.py", line 168, in <module>
    main()
  File "C:\Users\Mert\Desktop\di725-vision-language-models-for-image-captioning\train_with_wandb.py", line 158, in main
    trainer.train()
  File "C:\Users\Mert\AppData\Local\Programs\Python\Python39\lib\site-packages\transformers\trainer.py", line 2164, in train
    return inner_training_loop(
  File "C:\Users\Mert\AppData\Local\Programs\Python\Python39\lib\site-packages\transformers\trainer.py", line 2265, in _inner_training_loop
    self.create_optimizer_and_scheduler(num_training_steps=max_steps)
  File "C:\Users\Mert\AppData\Local\Programs\Python\Python39\lib\site-packages\transformers\trainer.py", line 1159, in create_optimizer_and_scheduler
    self.create_optimizer()
  File "C:\Users\Mert\AppData\Local\Programs\Python\Python39\lib\site-packages\transformers\trainer.py", line 1207, in create_optimizer
    optimizer_cls, optimizer_kwargs = self.get_optimizer_cls_and_kwargs(self.args, opt_model)
  File "C:\Users\Mert\AppData\Local\Programs\Python\Python39\lib\site-packages\transformers\trainer.py", line 1291, in get_optimizer_cls_and_kwargs
    for mapping in args.optim_args.replace(" ", "").split(","):
AttributeError: 'dict' object has no attribute 'replace'
Traceback (most recent call last):
  File "C:\Users\Mert\Desktop\di725-vision-language-models-for-image-captioning\train_with_wandb.py", line 168, in <module>
    main()
  File "C:\Users\Mert\Desktop\di725-vision-language-models-for-image-captioning\train_with_wandb.py", line 158, in main
    trainer.train()
  File "C:\Users\Mert\AppData\Local\Programs\Python\Python39\lib\site-packages\transformers\trainer.py", line 2164, in train
    return inner_training_loop(
  File "C:\Users\Mert\AppData\Local\Programs\Python\Python39\lib\site-packages\transformers\trainer.py", line 2265, in _inner_training_loop
    self.create_optimizer_and_scheduler(num_training_steps=max_steps)
  File "C:\Users\Mert\AppData\Local\Programs\Python\Python39\lib\site-packages\transformers\trainer.py", line 1159, in create_optimizer_and_scheduler
    self.create_optimizer()
  File "C:\Users\Mert\AppData\Local\Programs\Python\Python39\lib\site-packages\transformers\trainer.py", line 1207, in create_optimizer
    optimizer_cls, optimizer_kwargs = self.get_optimizer_cls_and_kwargs(self.args, opt_model)
  File "C:\Users\Mert\AppData\Local\Programs\Python\Python39\lib\site-packages\transformers\trainer.py", line 1291, in get_optimizer_cls_and_kwargs
    for mapping in args.optim_args.replace(" ", "").split(","):
AttributeError: 'dict' object has no attribute 'replace'
